# Random Forest vs Gradient boosting


Проект является переработкой задачи соревнования на платформе [kaggle](https://www.kaggle.com/code/dmitriyvishnyakov/eda-project-hotels-rating) о предсказании рейтинга отлелей компании Booking. Одна из проблем компании — это нечестные отели, которые накручивают себе рейтинг. Одним из способов нахождения таких отелей является построение модели, которая предсказывает рейтинг отеля. Если предсказания модели сильно отличаются от фактического результата, то, возможно, отель играет нечестно, и его стоит проверить.

Целью является сравнение двух типов моделей RandomForestRegressor и GragientBoostingRegressor

В датафрейме представлены следующие признаки:

+ hotel_address — адрес отеля;
+ review_date — дата, когда рецензент разместил соответствующий отзыв;
+ average_score — средний балл отеля, рассчитанный на основе последнего комментария за последний год;
+ hotel_name — название отеля;
+ reviewer_nationality — страна рецензента;
+ negative_review — отрицательный отзыв, который рецензент дал отелю;
+ review_total_negative_word_counts — общее количество слов в отрицательном отзыв;
+ positive_review — положительный отзыв, который рецензент дал отелю;
+ review_total_positive_word_counts — общее количество слов в положительном отзыве.
+ reviewer_score — оценка, которую рецензент поставил отелю на основе своего опыта;
+ total_number_of_reviews_reviewer_has_given — количество отзывов, которые рецензенты дали в прошлом;
+ total_number_of_reviews — общее количество действительных отзывов об отеле;
+ tags — теги, которые рецензент дал отелю;
+ days_since_review — количество дней между датой проверки и датой очистки;
+ additional_number_of_scoring — есть также некоторые гости, которые просто поставили оценку сервису, но не оставили отзыв. Это число указывает, сколько там действительных оценок без проверки.
+ lat — географическая широта отеля;
+ lng — географическая долгота отеля.

## Random Forest


Random Forest является самым распространённым ансамблем типа бэггинг, в ходе которого параллельно обучается множество одинаковых моделей.

Выборка в этой модели выбирается посредством бутстрапа. Далее каждое решающее дерево обучается на случайной подвыборке из признакового пространства. Иными словами, случайный лес содержит в себе две случайности: случайная выборка для обучения и случайное множество признаков.

После обучения отдельные решающие деревья объединяются в ансамбль. Для задачи регрессии ансамблирование происходит посредством усреднения результата предсказания каждой базовой модели.

Обучение модели на тренировочной и тестовой выборках показывают большой разброс в значениях внутри метрик. Разброс (variance) — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели. Таким образом, модель не смогла найти нужные закономерности и переобучилась. Особенно это хорошо видно на метрике $R^{2}$.

Попробуем решить проблему двумя способами:

1. Сократить количество признаков.
2. Подобрать другую модель.

Значимые признаки были отобраны тестом ANOVA  и корреляцией Пирсона. Метрики на обученом модели не показали значительного изменения.

## Gradient boosting

Бустинг (boosting) — это алгоритм построения ансамбля, основанный на последовательном построении слабых моделей, причём каждая новая модель пытается уменьшить ошибку предыдущей. После того как все модели обучены, они объединяются в композицию.

В отличие от бэггинга, бустинг обучается на одном и том же наборе данных, без генерации дополнительных выборок. Однако в процессе обучения меняются  веса наблюдений. Если слабая модель допустила ошибку на каких-то примерах, то вес этих примеров увеличивается и на них концентрируется следующая за ней модель.

Поскольку основная цель бустинга — уменьшение смещения, в качестве базовых моделей часто выбирают алгоритмы с высоким смещением и небольшим разбросом, например короткие деревья решений. У каждого из таких деревьев слабая предсказательная способность, но если их объединить, мы получим очень мощную модель.

Таким образом, построим модель основанную на деревьях решений, в таком же количестве n_estimators=100, и с небольшой глубиной max_depth=3. Для предотвращения переобучения увеличим темп обучения learning_rate=0.6.

Разброс значений метрик тренировочной и тестовой выборок значительно сократился, что может говорить о снижении переобучения. Однако, показатель метрики $R^{2}$ не достигает 0.5.

Поэтому возможно продолжение работы над улучшением качества модели за счет повторного преобразования признаков и подбора гиперпараметров.
